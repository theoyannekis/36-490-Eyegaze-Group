title: "eyegaze random forest"
output: 
  html_document:
    toc:  true
    toc_float:  true
    code_folding:  show
---

Random Forest provides us with insights on the importance of predictors of the model. Moreover, the random forest models generally have good predicting power, which makes it trustworthy.  


First step, we import the dataset. 

```{r}
eyegaze <- read.csv("Eyetracking_Data.csv",na.string=c("999","999.00","#NULL!"),stringsAsFactors = FALSE)
```

Here we include randomForest package from R library. Before we fit the model, some modifications to the dataset have to be made.  

```{r,message=FALSE,warning=FALSE}
library(randomForest)
library(dplyr)
#removing the NA values in the dataset
eyegaze.rf <- na.omit(eyegaze) 
#We do not care about id, dot and dob. So we remove them from dataset
eyegaze.rf$ID <- NULL
eyegaze.rf$DOT <- NULL
eyegaze.rf$DOB <- NULL
#Setting seed for random generator in R so the result is consistent every time we run the code
set.seed(110)
#Setting up the training and testing datasets
train.rf.index <- sample(1:nrow(eyegaze.rf), 0.8*nrow(eyegaze.rf))
test.rf <- eyegaze.rf[-train.rf.index, ]
```

After the setup is done, we fit the model with CompDiff against all other variables in the dataset. 

```{r}
#Fitting the model
rf.model <- randomForest(CompDiff ~ ., data = eyegaze.rf, 
                         subset = train.rf.index, importance = TRUE, mtry = 50)
#Note that "mtry"" is for controlling the number of variables to choose from for each split in the trees generated by random forest. This is usually chosen by looping over all possible mtry values, from 1 to number of predictors. In our case, as the difference in mtry is minimal across all mtry values, we set it to 50.
```

```{r}
rf.predicted <- predict(rf.model, test.rf)

rf.model
```

We examine the goodness-of-fit of the rf model by looking at the MSE, out of bag error. Both shows reasonably good fitness of the rf model. After testing on the testing dataset, we plot the fitted vs. actual value plot and sees reasonably reliable predicting power.  

```{r,warning=FALSE,message=FALSE}
library(ggplot2)
ggplot(test.rf, aes(x = rf.predicted, y = CompDiff)) + 
  geom_point() +
  coord_fixed()
```

Our goal is to analyze the variable importance in the rf model. Lucky for us, the random forest model provides us with a way to examine the importance of variables easily in R. The following code produces graphs of variable importance for all variables. Although by default it shows only top 20 most important variables.  

```{r}
varImpPlot(rf.model)
to.remove<-c(which(data.frame(rf.model$importance)$MeanDecreaseAccuracy == 
                     min(data.frame(rf.model$importance)$MeanDecreaseAccuracy)))
var.predict<-paste(names(eyegaze.rf)[-c(5,to.remove)],collapse="+")
rf.form <- as.formula(paste(names(eyegaze.rf)[5], var.predict, sep = " ~ "))

rf.model<-randomForest(rf.form,data=eyegaze.rf,importance=TRUE,ntree=100)

varImpPlot(rf.model)
```
rpart to visualize trees.
